---
title: "Homework 1"
author: "Kai Wang"
date: "2018/1/24"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
library(dplyr)
```

#1 Perceptron Algorithm and Convergence Analysis

##1

(a) The boolean function is $y = f(x_1,x_2) = \mathbb{I}_{x_1+x_2-1>0}$.

```{r}
df1 = data.frame(x1 = c(0,1,0,1), x2 = c(0,0,1,1), y = as.factor(c(0,1,1,1)))
ggplot(df1) + geom_point(aes(x = x1, y = x2, col = (x1+x2>1.5)))+
  theme_bw() + geom_abline(slope = -1, intercept = 1.5, col = "red")
```

(b) The boolean function is $y = f(x_1,x_2) = \mathbb{I}_{x_1+x_2=1}$. Such a boolean function cannot be represented by a perceptron.

```{r}
df2 = data.frame(x1 = c(0,1,0,1), x2 = c(0,0,1,1), y = as.factor(c(0,1,1,1)))
ggplot(df2) + geom_point(aes(x = x1, y = x2, col = (x1+x2==1)))+
  theme_bw() 
```

(c) The boolean function is $y = f(x_1,x_2) = \mathbb{I}_{x_1+x_2+x_3>2}$. Note that (1,1,1) is the only positive point.

```{r}
library(scatterplot3d)
df3 = data.frame(x1 = c(0,0,0,0,1,1,1,1), x2 = c(0,0,1,1,0,0,1,1),x3=c(0,1,0,1,0,1,0,1), y = as.factor(c(0,0,0,0,0,0,0,1)))
s3d <- scatterplot3d(df3[,-4], pch = 16, xlim = c(-1, 2), ylim = c(-1, 2), zlim = c(-2, 2))
s3d$plane3d(Intercept = 2 , x.coef = -1, y.coef = -1, "solid", col="red")
```

##2

Let $z$ be a random point on the decision boundary. We have $f(z) = \beta_0+\beta^Tz = 0$. So we have $\beta^Tz = -\beta_0$. We know that the distance of a point to a vector equals to the product of those two vectors.

$$\begin{aligned}
\textit{distance} &= \frac{|\overrightarrow{\beta}(\overrightarrow{x}-\overrightarrow{z})|}{||\beta||_2}\\
&= \frac{|\overrightarrow{\beta}\overrightarrow{x}+\beta_0|}{||\beta||_2}\\
&= \frac{|f(x)|}{||\beta||_2}\\ 
&= \frac{1}{||\beta||_2}yf(x) 
\end{aligned}$$

Because $y$ and $f(x)$ have the same sign, $yf(x) = |f(x)|$.

##3

$$w^{(T)}\cdot w^{(sep)} - w^{(T-1)}\cdot w^{(sep)} = y_i x_i w^{(sep)} \geq 1$$
$$(w^{(T)}-w^{(0)})\cdot w^{(sep)} = \sum_{t=1}^T((w^{(T)}-w^{(T-1)})\cdot w^{(sep)})\geq T$$
Hence, $$T\leq (w^{(T)}-w^{(0)})\cdot w^{(sep)}\leq ||w^{(T)}-w^{(0)}||_2||w^{(sep)}||_2 \leq ||w^{(T)}-w^{(0)}||_2$$

Since we have $T>1$ and $T\leq ||w^{(T)}-w^{(0)}||_2$, we have 
$T\leq  ||w^{(T)}-w^{(0)}||_2^2$.

When the perceptron algorithm converges to a separating plane, we have $T\leq  ||w^{(sep)}-w^{(0)}||_2^2$, which is equivalent to $T\leq  ||w^{(0)}-w^{(sep)}||_2^2$



#2 Programming Assignment

##1
```{r echo=FALSE}
##download dataset
##reference: https://stackoverflow.com/questions/21521571/how-to-read-mnist-database-in-r
library(darch)
provideMNIST(download = T)
load("data/train.RData")
load("data/test.RData")

#filter out 4/9 for all datasets, the label starts with 0
train.x = trainData[c(which(trainLabels[,5]==1),which(trainLabels[,10]==1)),]
train.label = trainLabels[c(which(trainLabels[,5]==1),which(trainLabels[,10]==1)),]
test.x = testData[c(which(testLabels[,5]==1),which(testLabels[,10]==1)),]
test.label = testLabels[c(which(testLabels[,5]==1),which(testLabels[,10]==1)),]

#shuffle data
k = sample(nrow(train.x))
train.x = train.x[k,]
train.label = train.label[k,]

l = sample(nrow(test.x))
test.x = test.x[l,]
test.label = test.label[l]

#change label from 0 to -1
train.label = ifelse(train.label[,5]==1, 1, -1)
test.label = ifelse(test.label[,5]==1, 1, -1)
```

###a
```{r}
#reference: https://rpubs.com/FaiHas/197581

perceptron = function(x, y, epoch) {
        # initialize weight vector
        weight = rep(0, dim(x)[2])
        result = matrix(0, nrow = epoch, ncol = dim(x)[2])
        for (i in 1:epoch) {
                for (j in 1:length(y)) {
                        z = sum(weight*x[j, ])
                        if(z <= 0) {
                                ypred = -1
                        } else {
                                ypred = 1
                        }
                        
                        # Update weight
                        if (y[j] != ypred) {
                          weight = weight + y[j] * x[j,]
                        }
                }
          #save weight vectors for each step
          result[i,] = weight
        }
        return(result)
}
```

```{r}
result = perceptron(train.x, train.label,100)
pred.train = t(train.x %*% t(result))
pred.train[pred.train>0] = 1
pred.train[pred.train<=0] = -1

accuracy = apply(pred.train, 1, function(x) {return(sum(x==train.label)/length(train.label))})
accuracy = c(0,accuracy)
#plot accuracy vs. epoch
plot(x = 0:100, y=accuracy, ylim = range(0,1), type = "l", col = "blue", xlab = "epoch", ylab = "accuracy")
```

The accuracy does not seem too good and convergence has not achieved. We could increase the epoch. By increasing epoch, I found that accuracy increased until about epoch = 500. The final training accuracy is aroun .80.

```{r}
#increase epoch
result = perceptron(train.x, train.label,100)
pred.train = t(train.x %*% t(result))
pred.train[pred.train>0] = 1
pred.train[pred.train<=0] = -1

accuracy = apply(pred.train, 1, function(x) {return(sum(x==train.label)/length(train.label))})
accuracy = c(0,accuracy)
#plot accuracy vs. epoch
plot(x = 0:500, y=accuracy, ylim = range(0,1), type = "l", col = "blue", xlab = "epoch", ylab = "accuracy")
```


###b
```{r}
#test accuracy
result.test = perceptron(test.x,test.label,100)
pred.test = t(test.x %*% t(result.test))
pred.test[pred.test>0] = 1
pred.test[pred.test<=0] = -1

accuracy = apply(pred.test, 1, function(x) {return(sum(x==test.label)/length(test.label))})
accuracy = c(0,accuracy)
#plot accuracy vs. epoch
plot(x = 0:100, y=accuracy, ylim = range(0,1), type = "l", col = "blue", xlab = "epoch", ylab = "accuracy")
```

The test accuracy is much higher than training accuracy and it converges quicker. The test accuracy is around .95 after converging.

###c
```{r}
#create confusion matrix
confusion = function(pred, true){
  TP = sum(pred == 1 & true == 1)
  TN = sum(pred == -1 & true == -1)
  FP = sum(pred == 1 & true == -1)
  FN = sum(pred == -1 & true == 1)
  df = data.frame(matrix(c(TP,FN,FP,TN),2))
  colnames(df) = c("Positive","Negative")
  rownames(df) = c("Predicted Positive","Predicted Negative")
  return(df)
}

library(knitr)
kable(confusion(pred.test[100,],test.label))
```

The accuracy is $\frac{857+1009}{857+0+125+1009} = 0.9372$.

###d
```{r}
#
perceptron2 = function(x, y, epoch) {
        # initialize weight vector and error vector
        weight = rep(0, dim(x)[2])
        result = matrix(0, nrow = epoch, ncol = dim(x)[2])
        for (i in 1:epoch) {
                for (j in 1:length(y)) {
                        z = sum(weight*x[j, ])
                        if(z <= 0) {
                                ypred = -1
                        } else {
                                ypred = 1
                        }
                        
                        # Update weight
                        if (y[j] != ypred) {
                          weight = weight + y[j] * x[j,]
                        }
                        
                        if(j == floor(length(y)/3)){
                          weight_13 = weight
                        }
                }
          #save weight vectors for each step
          result[i,] = weight
        }
        result = list(result,weight_13)
        return(result)
}
```

```{r}
result2 = perceptron2(train.x, train.label,100)
weight = result2[[1]]
weight_13 = result2[[2]]
pred.train = t(train.x %*% t(weight))
pred.train[pred.train>0] = 1
pred.train[pred.train<=0] = -1

pred_13 = train.x %*% weight_13
pred_13[pred_13>0] = 1
pred_13[pred_13<=0] = -1


roc <- function(weight, b){
  ROC = data.frame(matrix(0, nrow = length(b), ncol = 2))
  for (i in 1:length(b)){
    pred = train.x %*% weight
    pred = ifelse(pred > b[i], 1, -1)
    TP = sum(pred == 1 & train.label == 1)
    TN = sum(pred == -1 & train.label == -1)
    FP = sum(pred == 1 & train.label == -1)
    FN = sum(pred == -1 & train.label == 1)
    TPR = TP/(TP+FN)
    FPR = FP/(FP+TN)
    ROC[i,1] = FPR
    ROC[i,2] = TPR
  }
  return(ROC)
}

b = seq(-800, 800, 5)

roc_complete = roc(weight[100,], b)
roc_13 = roc(weight_13, b)
plot(roc_complete$X1, roc_complete$X2, type = "l", col = "blue", xlab = "FPR", ylab = "TPR")
lines(roc_13$X1, roc_13$X2, col = "red")
legend("topright", legend = c("full","1/3"),col = c("blue","red"), lty = 1)
```

From the ROC curve, we can see running algorithm until convergence has a slightly better performance, which means it has a better decision boundary.


###e

```{r}
auc = function(roc){
  colnames(roc) = c("TPR","FPR")
  roc = arrange(roc, TPR)
  s = 0
  for (i in 1:(nrow(roc)-1)){
    s = s + (roc$TPR[i] + roc$TPR[i+1])*(roc$FPR[i+1] - roc$FPR[i])/2
  }
  return(1-s)
}

AUC_complete = auc(roc_complete)
AUC_complete
AUC_13 = auc(roc_13)
AUC_13
```

The AUC of $w^*$ is greater than the AUC of $w'$.

##2

###a

```{r}
#Balanced Window algorithm
BW = function(x,y,epoch,eta){
        p = dim(x)[2] #number of parameters
        wp = rep(1/(2*p), p)
        wn = rep(1/(2*p), p)
        result = matrix(0, nrow = epoch, ncol = 2*dim(x)[2])
        for (i in 1:epoch) {
                for (j in 1:length(y)) {
                        z = y[j] * sum((wp-wn) * x[j, ])
                        if(z <= 0) {
                          wp = wp * exp(eta*y[j]*x[j, ])
                          wn = wn * exp(-eta*y[j]*x[j, ])
                          s = wn+wp
                          wp = wp/s
                          wn = wn/s #update weights
                        }
                }
          #save weight vectors for each step
          result[i,] = c(wp,wn)
        }
        return(result)
}
```

```{r}
result.bwtrain = BW(train.x, train.label, 100, eta = 0.1)
pred.train = result.bwtrain %*% t(cbind(train.x,-1*train.x))
pred.train[pred.train>0] = 1
pred.train[pred.train<=0] = -1

accuracy = apply(pred.train, 1, function(x) {return(sum(x==train.label)/length(train.label))})
accuracy = c(0,accuracy)
#plot accuracy vs. epoch
plot(x = 0:(length(accuracy)-1), y=accuracy, ylim = range(0,1), type = "l", col = "blue", xlab = "epoch", ylab = "accuracy")
```

```{r}
result.bwtest = BW(test.x, test.label, 100, eta = 0.1)
pred.test = result.bwtest %*% t(cbind(test.x,-1*test.x))
pred.test[pred.test>0] = 1
pred.test[pred.test<=0] = -1

accuracy = apply(pred.test, 1, function(x) {return(sum(x==test.label)/length(test.label))})
accuracy = c(0,accuracy)
#plot accuracy vs. epoch
plot(x = 0:(length(accuracy)-1), y=accuracy, ylim = range(0,1), type = "l", col = "blue", xlab = "epoch", ylab = "accuracy")

#confusion matrix
kable(confusion(pred.test[100,],test.label))
```

The test accuracy is $\frac{TP+TN}{TP+TN+FN+FP} = \frac{789+1009}{789+193+0+1009} = 0.9031$.


###b

Based on my experiments, $\eta = 0.1$ seems to be the best and $\eta = 0.5$ also performs very close. The best way to decide which value to use for $\eta$ is create a range of $\eta$ and calculate some sort of loss function and find a value to optimize the loss function.

