---
title: "Homework2"
author: "Kai Wang"
date: "2018/1/30"
output: pdf_document

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(caret)
library(rpart.plot)
```

#1 Classifiers for Basketball Courts

##a

```{r}
#oberserved shots
shots = data.frame(x1=c(.75,.85,.85,.15,.05,.05,.85),
                   x2=c(.10,.80,.95,.10,.25,.50,.25),
                   y=c(-1,-1,1,-1,1,1,-1))
shots.data = shots[,c(1,2)] %>% as.matrix()
shots.label = shots[,3] %>% as.matrix()

#perceptron algorithm from HW1
perceptron = function(x, y, epoch) {
        # initialize weight vector
        weight = rep(0, dim(x)[2])
        result = matrix(0, nrow = epoch, ncol = dim(x)[2])
        for (i in 1:epoch) {
                for (j in 1:length(y)) {
                        z = sum(weight*x[j, ])
                        if(z <= 0) {
                                ypred = -1
                        } else {
                                ypred = 1
                        }
                        
                        # Update weight
                        if (y[j] != ypred) {
                          weight = weight + y[j] * x[j,]
                        }
                }
          #save weight vectors for each step
          result[i,] = weight
        }
        return(result)
}
```

```{r}
weight = perceptron(shots.data, shots.label,100)
pred.train = t(shots.data %*% t(weight))
pred.train[pred.train>0] = 1
pred.train[pred.train<=0] = -1
accuracy = apply(pred.train, 1, function(x) {return(sum(x==shots.label)/length(shots.label))})
accuracy = c(0,accuracy)
#plot accuracy vs. epoch
plot(x = 0:100, y=accuracy, ylim = range(0,1), type = "l", col = "blue", xlab = "epoch", ylab = "accuracy")
```

From the epoch vs. accuracy plot, we can see the perceptron converges at 37th iteration. Since the accuracy is 100%, there's no empirical error for this classifier. We can come up with other linear classifiers which will give the same error(0). For example, an boolean function $y = f(x_1,x_2) = \mathbb{I}_{-x_1+x_2>0}$. More generally, as long as the slope of the linear classifier passing through origin is between $(\frac{16}{17},\frac{19}{17})$.

```{r}
#plot observed data and another seperation line
ggplot(shots) + geom_point(aes(x = x1, y = x2, col = as.factor(y)))+
  theme_bw() + geom_abline(slope = -weight[100,1]/weight[100,2], color = 'red') + 
  geom_abline(slope = 1, intercept = 0, color = 'blue') + 
  stat_function(fun=function(x) sqrt(x), xlim = c(0,1))
```

The above plot shows data and the decision boundary of the perceptron (as red line). In addition, the boolean function $y = f(x_1,x_2) = \mathbb{I}_{-x_1+x_2+0.08>0}$ is also plotted as a black line, which clearly seperates our data with no empirical error.

##b

```{r}
#split the decision tree based on gini index
shots = shots %>% 
  mutate(x_1 = ifelse(x1>0.1, 1, -1) %>% as.factor(),
         x_2 = ifelse(x2>0.85, 1, -1) %>% as.factor())
fit1 = rpart(as.factor(y)~x_1+x_2, data = shots, method = "class", parms = list(split = "gini"), 
             control=rpart.control(minsplit=1))
summary(fit1)
rpart.plot(fit1)
```

We can use "rpart" package to split our decision tree based on gini index. And this decision tree has all data points correctly classified.

```{r}
ggplot(shots) + geom_point(aes(x = x1, y = x2, col = as.factor(y)))+
  theme_bw() + geom_vline(xintercept = 0.1, color = "red") + geom_hline(yintercept = 0.85, color = "blue") +
  stat_function(fun=function(x) sqrt(x), xlim = c(0,1))
```

We can choose $x_1 > 0.1$ and $x_2 > 0.85$ as our threshold for $x_1,x_2$, then split the decision tree based on transformed data. Using the reduction in the Gini index as the splitting criterion, the error is 0. We can adjust the threshold for $x_1$ between $[0.05,0.15)$ and for $x_2$ bewtween $[0.80,0.95)$, which will give us the same split on the data. The decision tree of such a threshold is plotted as follow.

```{r}
shots = shots %>% 
  mutate(x_1 = ifelse(x1>0.08, 1, -1) %>% as.factor(),
         x_2 = ifelse(x2>0.90, 1, -1) %>% as.factor())
fit2 = rpart(as.factor(y)~x_1+x_2, data = shots, method = "class", parms = list(split = "gini"), 
             control=rpart.control(minsplit=1))
summary(fit2)
rpart.plot(fit2)
```

##c

Since the three-point line is our approximation is $x_2 = \sqrt{x_1}$, we know $f(x)^{\text{true}} = \mbox{sign}(x_2-\sqrt{x_1})$. We need to use a linear classifier that goes through origin, say $f(\textbf{x}) = \mbox{sign}(x_2-ax_1)$.

The true risk is $R^\text{true}(f) = \mathbb{E}_{(x,y)\sim D}l(f(\textbf{x}),y)$, with the misclassification function $l(f(\textbf{x}),y)=1_{\mbox{sign}(f(\textbf{x})\neq y)}$.

Since $x_1,x_2\sim \text{Uniform}[0,1]$, we transform this problem into finding the definite integral of area between $x_2=\sqrt{x_1}$ and $x_2 = ax_1$, and it's intuitive that $a\geq1$.

$$\begin{aligned}
R^\text{true}(f) &= \int_0^{\frac{1}{a^2}}(\sqrt{x_1}-ax_1)dx_1+\int_{\frac{1}{a^2}}^{\frac{1}{a}}(ax_1-\sqrt{x_1})dx_1+\int_{\frac{1}{a}}^1 (1-\sqrt{x_1})dx_1\\
&= \frac{2}{3}a^{-3}-\frac{a}{2}a^{-4}+\frac{a}{2}a^{-2}-\frac{a}{2}a^{-4}-\frac{2}{3}a^{-\frac{3}{2}}+\frac{2}{3}a^{-3}+1-\frac{1}{a}-\frac{2}{3}+\frac{2}{3}a^{-\frac{3}{2}}\\
&= \frac{1}{3}a^{-3}-\frac{1}{2}a^{-1}+\frac{1}{3}
\end{aligned}$$

To minimize the true risk, we need to choose a value of $a$ so that $R^\text{true}(f)$ is minimized.

We have $\frac{dR^\text{true}(f)}{da} = \frac{1}{2a^2}-\frac{1}{a^4}$, which equals to 0 when $a=\sqrt{2}$. The second derivative shows that $R^\text{true}(f)$ is increasing after $a = \sqrt{2}$. Hence, $R^\text{true}(f)_\text{min}=\frac{1}{3}-\frac{\sqrt{2}}{6}$.

```{r}
#plot observed data and another seperation line
ggplot(shots) + geom_point(aes(x = x1, y = x2, col = as.factor(y)))+
  theme_bw() + geom_abline(slope = -weight[100,1]/weight[100,2], color = 'red') + 
  geom_abline(slope = sqrt(2), intercept = 0, color = 'blue') +
  stat_function(fun=function(x) sqrt(x), xlim = c(0,1))
```

The blue line shows the optimal linear classifier, but empirically it classified 1 point wrong. The empirical error $R(f) = \frac{1}{8}$. Any linear classifier with slope between $(\frac{2}{3},5)$ would result in the same empirical error.

##d

```{r}
fit3 = rpart(as.factor(y)~x1+x2, data = shots, method = "class", parms = list(split = "gini"), 
             control=rpart.control(minsplit=1))
summary(fit3)
rpart.plot(fit3)
```

```{r}
ggplot(shots) + geom_point(aes(x = x1, y = x2, col = as.factor(y)))+
  theme_bw() + geom_vline(xintercept = 0.1, color = "red") + geom_hline(yintercept = 0.88, color = "blue") + 
  stat_function(fun=function(x) sqrt(x), xlim = c(0,1))
```

First decision tree splits on $x_1$ first and the $x_2$, we can see $s_1 = 0.1$ and $s_3 = 0.88$. $s_2$ does not exist since there is no split with $x_1<= 0.1$. This is among the solutions that achieved the minimum empirical error.


```{r}
#split on x2 first by assigning higher cost to x1
fit4 = rpart(as.factor(y)~x1+x2, data = shots, method = "class", parms = list(split = "gini"), 
             control=rpart.control(minsplit=1, maxdepth = 2), cost = c(2,1))
summary(fit4)
rpart.plot(fit4)
```

```{r}
#plot second tree
ggplot(shots) + geom_point(aes(x = x1, y = x2, col = as.factor(y)))+
  theme_bw() + geom_vline(xintercept = 0.45, color = "red") + geom_hline(yintercept = 0.17, color = "blue") +
  stat_function(fun=function(x) sqrt(x), xlim = c(0,1))
```

Second decision tree splits on $x_2$ first and the $x_1$, we can see $s_1 = 0.17$ and $s_2 = 0.45$. $s_3$ does not exist since there is no split with $x_2<= 0.17$. This is not among the solutions that achieved the minimum empirical error.

##e

Since we know the true function $f(\textbf{x}) = \mathbb{I}_{x_2 - \sqrt{x_1}>0}$, we can make a tranformation $x_2 = \sqrt{x_1}$. Then our optimal linear classifier will simply be $y = \mathbb{I}_{x_2 - x_1>0}$ and its error is 0.

```{r}
#plot with transformation on x1
shots = shots %>% 
  mutate(x1t = sqrt(x1))
ggplot(shots) + geom_point(aes(x = x1t, y = x2, col = as.factor(y)))+
  theme_bw() + geom_abline(slope = 1, color = 'blue')
```

##f

```{r}
fit5 = rpart(as.factor(y)~x1t+x2, data = shots, method = "class", parms = list(split = "gini"), 
             control=rpart.control(minsplit=1))
summary(fit5)
rpart.plot(fit5)
```

```{r}
ggplot(shots) + geom_point(aes(x = x1t, y = x2, col = as.factor(y)))+
  theme_bw() + geom_vline(xintercept = 0.31, color = "red") + geom_hline(yintercept = 0.88, color = "blue")
```

After the transformation on $x_1$, the decision tree achieved the same error.

##h

We can represent the paint as area where $x_1\geq 0.5, x_2\leq 0.25$.

The true risk is $R^\text{true}(f) = \mathbb{E}_{(x,y)\sim D}l(f(\textbf{x}),y)$, with the misclassification function $l(f(\textbf{x}),y)=1_{\mbox{sign}(f(\textbf{x})\neq y)}$.

Since $x_1,x_2\sim \text{Uniform}[0,1]$, we transform this problem into finding the definite integral of area between 
$\begin{aligned}
x_2=
\begin{cases}
0.25, &x_1\geq 0.5\cr 0, &x_2<0.5 \end{cases}
\end{aligned}$
and $x_2 = ax_1$, and it's intuitive that $a\leq \frac{1}{2}$.

$$\begin{aligned}
R^\text{true}(f) &= \int_0^{\frac{1}{2}}ax_1dx_1 + \int_{\frac{1}{2}}^{\frac{1}{4a}}(\frac{1}{4}-ax_1)dx_1 + \int_{\frac{1}{4a}}^1 (ax_1-1)dx_1\\
&= \frac{3}{4}a+\frac{1}{4a}-\frac{9}{8}
\end{aligned}$$

To minimize $R^\text{true}(f)$, we have $\frac{dR^\text{true}(f)}{da} = \frac{3}{4} - \frac{1}{4a^2} = 0$, which means $a=\frac{\sqrt{3}}{3}$. Since $R^\text{true}(f)$ is decreasing on $[\frac{1}{2},\frac{\sqrt{3}}{3}]$ and increasing afterwards, we have $R^\text{true}(f)_{\text{min}} = \frac{\sqrt{3}}{2}-\frac{9}{8}$.

##i

The optimal decision tree will have decision boundaries that replicates the shape of paint and the error is 0.

```{r echo=FALSE, out.width='50%', out.extra='angle=90'} 
knitr::include_graphics('./tree.jpg')
```


#2

##a

###i
```{r}
#read in datasets
train = read.csv("train.csv")
test = read.csv("test.csv")
```

```{r}
#best split
stump1 = rpart(Y~., data = train, method = "class", parms = list(split = "gini"), 
             control=rpart.control(minsplit=1, maxdepth = 1, maxsurrogate = 0))
summary(stump1)
rpart.plot(stump1)
```

```{r}
#best surrogate split
surrogate_split = function(x){
  stump = rpart(Y~., data = x, method = "class", parms = list(split = "gini"), 
             control=rpart.control(minsplit=1, maxdepth = 1, maxsurrogate = 0))
  n = dim(x)[1]
  lambda = rep(NA, dim(x)[2]-2)
  i = 1
  bestsplit = names(stump$variable.importance)
  iter = setdiff(names(x),c("Y",bestsplit))
  for (j in iter){
    pL = length(which(x[ ,bestsplit] == 0))/n
    pR = length(which(x[ ,bestsplit] == 1))/n
    pLL = length(which(x[ ,bestsplit] == 0 & x[,j] == 0))/n
    pRR = length(which(x[ ,bestsplit] == 1 & x[,j] == 1))/n
    lambda[i] = (min(pL,pR) - (1-pLL-pRR))/min(pL,pR)
    i = i + 1
  }
  best_surrogate = rpart(Y~., data = x[ ,c(iter[which.max(lambda)],"Y")], 
                         method = "class", parms = list(split = "gini"), 
                         control=rpart.control(minsplit=1, maxdepth = 1, maxsurrogate = 0))
  return(best_surrogate)
}

surrogate_split(train)
```


