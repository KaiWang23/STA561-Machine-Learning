{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1 Separability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For points in the convex hull of $\\{X_n\\}$, we can calculate its linear discriminants as $y(x) = w^T X + w_0$.\n",
    "\n",
    "Sine $X=\\sum_n \\alpha_n X_n$, we have $y(x) = w^T (\\sum_n \\alpha_n X) + w_0$.\n",
    "\n",
    "And since $\\sum_n \\alpha_n = 1$, we have $y(x) = \\alpha_n(\\sum_n  w^T X+ w_0)>0$.\n",
    "\n",
    "Similarly, the points in the convex hull $\\{X_m'\\}$, we have $y(x') = \\alpha_n'(\\sum_n  w^T X'+ w_0)<0$.\n",
    "\n",
    "The intersection of these two convex hulls are $y(xx')=\\alpha_n(\\sum_n  w^T X+ w_0)= \\alpha_n'(\\sum_n  w^T X'+ w_0)$, however, $y(xx')$ cannot be greater than 0 and less than 0 at the same time, so $y(xx')$ does not exist. So when two sets of points are linearly seperable, their convex hulls do not intersect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2 Logistic regression and gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a\n",
    "\n",
    "$\\sigma ' (a)= \\frac{e^{-a}}{(1+e^{-a})^2}$.\n",
    "\n",
    "$\\begin{aligned}\n",
    "\\sigma(a)(1-\\sigma(a))&=\\frac{1}{1+e^{-a}}\\frac{e^{-a}}{1+e^{-a}}\\\\\n",
    "&=\\frac{e^{-a}}{(1+e^{-a})^2}\\\\\n",
    "&=\\sigma ' (a)\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### b\n",
    "\n",
    "$\\begin{aligned}\n",
    "L_w (\\{(x^{(i)},y^{(i)})\\}_{i=1}^n) &= \\sum_{i=1}^n \\{-y^{(i)} \\log[h_w (x^{(i)})] -(1-y^{(i)})\\log[1-h_w(x^{(i)})]\\}\\\\\n",
    "&= \\sum_{i=1}^n \\{-y^{(i)} \\log [\\frac{1}{1+e^{-w^T x^{(i)}}}]-(1-y^{(i)})\\log[1-\\frac{1}{1+e^{-w^T x^{(i)}}}]\\}\n",
    "\\end{aligned}$\n",
    "\n",
    "$\\begin{aligned}\n",
    "\\frac{\\partial L_w (\\{(x^{(i)},y^{(i)})\\}_{i=1}^n)}{\\partial w_j} &= \n",
    "\\frac{\\partial \\sum_{i=1}^n \\{-y^{(i)} \\log [\\frac{1}{1+e^{-w^T x^{(i)}}}]-(1-y^{(i)})\\log[1-\\frac{1}{1+e^{-w^T x^{(i)}}}]\\}}{\\partial w_j} \\\\\n",
    "&= \\sum_{i=1}^n -y^{(i)}({1+e^{-w^T x^{(i)}}}) \\frac{e^{-w^T x^{(i)}}x^{(i)}}{(1+e^{-w^T x^{(i)}})^2}-(1-y^{(i)})\\frac{{1+e^{-w^T x^{(i)}}}}{{e^{-w^T x^{(i)}}}}\\frac{e^{-w^T x^{(i)}}x^{(i)}e^{-w^T x^{(i)}}-x^{(i)}e^{-w^T x^{(i)}}(1+e^{-w^T x^{(i)}})}{(1+e^{-w^T x^{(i)}})^2}\\\\\n",
    "&= \\sum_{i=1}^n -y^{(i)}\\frac{e^{-w^T x^{(i)}}x^{(i)}}{1+e^{-w^T x^{(i)}}}-(1-y^{(i)})\\frac{x^{(i)}e^{-w^T x^{(i)}}-x^{(i)}(1+e^{-w^T x^{(i)}})}{1+e^{-w^T x^{(i)}}}\\\\\n",
    "&= \\sum_{i=1}^n -y^{(i)}\\frac{e^{-w^T x^{(i)}}x^{(i)}}{1+e^{-w^T x^{(i)}}}+(1-y^{(i)})\\frac{x^{(i)}}{1+e^{-w^T x^{(i)}}}\\\\\n",
    "&= \\sum_{i=1}^n -y^{(i)}\\frac{e^{-w^T x^{(i)}}x^{(i)}}{1+e^{-w^T x^{(i)}}}+(1-y^{(i)})\\frac{x^{(i)}}{1+e^{-w^T x^{(i)}}}\\\\\n",
    "&= \\sum_{i=1}^n \\frac{(1-y^{(i)}-y^{(i)}e^{-w^T x^{(i)}})x^{(i)}}{1+e^{-w^T x^{(i)}}}\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### c\n",
    "\n",
    "A function f(x) is said to be convex if the following inequality holds true: $f\\left( \\alpha x + \\left( 1 - \\alpha\\right) y \\right) \\leq \\alpha f\\left( x \\right) + \\left( 1 - \\alpha\\right) f\\left( y \\right)$.\n",
    "\n",
    "So we can prove that $-\\log[h_w (x)]$ and $-\\log[1-h_w(x)]$ are convex instead.\n",
    "\n",
    "$\\begin{aligned}\n",
    "\\nabla_w -\\log[h_w (x)] &= \\nabla_w[\\log(1+e^{-w^T x}]\\\\\n",
    "&= \\frac{-e^{-w^T x}}{1+e^{-w^T x}}x\\\\\n",
    "&= (\\frac{1}{1+e^{-w^T x}}-1)x\\\\\n",
    "&= (h_w(x)-1)x\n",
    "\\end{aligned}$\n",
    "\n",
    "$\\begin{aligned}\n",
    "\\nabla_w^2 -\\log[h_w (x^{(i)})] &= \\nabla_w^2[\\log(1+e^{-w^T x})]\\\\\n",
    "&= \\nabla_w(h_w(x)-1)x\\\\\n",
    "&=h_w(x)(1-h_w(x))xx^T\n",
    "\\end{aligned}$\n",
    "\n",
    "$\\forall z: z^T \\nabla_w^2 -\\log[h_w (x)] = z^T[h_w(x)(1-h_w(x))xx^T]z = h_w(x)(1-h_w(x))(x^T z)^2 \\geq 0$, so the hessian matrix is positive semi-definite, so we proved that $-\\log[h_w (x^{(i)})]$ is a convex function.\n",
    "\n",
    "Similarly, \n",
    "\n",
    "$\\begin{aligned}\n",
    "\\nabla_w^2 -\\log[1-h_w(x)] &= \\nabla_w(\\nabla_w(-\\log(1-\\frac{1}{e^{-w^T x}}))\\\\\n",
    "&= \\nabla_w(x+\\nabla_w [\\log(1+e^{-w^T x})])\\\\\n",
    "&= \\nabla_w^2[-\\log(h_w (x))]\n",
    "\\end{aligned}$\n",
    "\n",
    "We already showed that this hassian matrix is positive semi-definie, so $-\\log[1-h_w(x)]$ is also convex. Hence the cross-entropy loss is also convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "products = pd.read_csv('amazon_baby_small.csv')\n",
    "products['high_rating'] = (products['rating'] > 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('important_words.json', 'r') as f: # Reads the list of most frequent words\n",
    "    important_words = json.load(f)\n",
    "important_words = [str(s) for s in important_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products = products.fillna({'review':''})  # fill in N/A's in the review column\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    import string\n",
    "    return text.replace(string.punctuation, ' ') \n",
    "\n",
    "products['review_clean'] = products['review'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word in important_words:\n",
    "    products[word] = products['review_clean'].apply(lambda s : s.split().count(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2808\n"
     ]
    }
   ],
   "source": [
    "products['contains_perfect'] = products['perfect'].apply(lambda x: 1 if x>=1 else 0)\n",
    "count = np.sum(list(products['contains_perfect']))\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products = products.drop(['name', 'review', 'review_clean', 'rating'], axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_numpy_data(dataframe, features, label):\n",
    "    dataframe['intercept'] = 1\n",
    "    features = ['intercept'] + features\n",
    "    feature_matrix = np.array(dataframe[features])\n",
    "    label_array = np.array(dataframe[label])\n",
    "    return(feature_matrix, label_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_matrix, high_rating = get_numpy_data(products, important_words, 'high_rating') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "produces probablistic estimate for P(y_i = +1 | x^(i), w).\n",
    "estimate ranges between 0 and 1.\n",
    "'''\n",
    "def predict_probability(feature_matrix, coefficients):\n",
    "    # Take dot product of feature_matrix and coefficients \n",
    "    # YOUR CODE HERE\n",
    "    scores = np.dot(feature_matrix, coefficients)\n",
    "    \n",
    "    # Compute the conditional probability\n",
    "    # YOUR CODE HERE\n",
    "    predictions = 1. / (1. + np.exp(-scores))\n",
    "    \n",
    "    # return predictions\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following outputs must match \n",
      "------------------------------------------------\n",
      "correct_predictions           = [0.98201379 0.26894142]\n",
      "output of predict_probability = [0.98201379 0.26894142]\n"
     ]
    }
   ],
   "source": [
    "dummy_feature_matrix = np.array([[1.,2.,3.], [1.,-1.,-1]])\n",
    "dummy_coefficients = np.array([1., 3., -1.])\n",
    "\n",
    "correct_scores      = np.array( [ 1.*1. + 2.*3. + 3.*(-1.),          1.*1. + (-1.)*3. + (-1.)*(-1.) ] )\n",
    "correct_predictions = np.array( [ 1./(1+np.exp(-correct_scores[0])), 1./(1+np.exp(-correct_scores[1])) ] )\n",
    "\n",
    "print ('The following outputs must match ')\n",
    "print ('------------------------------------------------')\n",
    "print ('correct_predictions           =', correct_predictions)\n",
    "print ('output of predict_probability =', predict_probability(dummy_feature_matrix, dummy_coefficients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_derivative(errors, feature):     \n",
    "    # Compute the dot product of errors and feature\n",
    "    derivative = np.dot(errors, feature)\n",
    "    \n",
    "    # Return the derivative\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_log_likelihood(feature_matrix, high_rating, coefficients):\n",
    "        \n",
    "    scores = np.dot(feature_matrix, coefficients)\n",
    "    sig = 1./(1. + np.exp(-scores))\n",
    "    \n",
    "    ## YOUR CODE HERE, do check overflow/underflow problem.\n",
    "    lp = np.sum(high_rating*np.log(sig) + (1-high_rating)*np.log(1-sig))\n",
    "    \n",
    "    return lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following outputs must match \n",
      "------------------------------------------------\n",
      "correct_log_likelihood           = -4.331411615436032\n",
      "output of compute_log_likelihood = -4.331411615436033\n"
     ]
    }
   ],
   "source": [
    "# here uses another way to compute log-likelihood. Don't worry about it too much.\n",
    "\n",
    "dummy_feature_matrix = np.array([[1.,2.,3.], [1.,0.,0]])\n",
    "dummy_coefficients = np.array([1., 3., -1])\n",
    "dummy_high_rating = np.array([0, 1])\n",
    "\n",
    "correct_indicators  = np.array( [ 0==+1,                                       1==+1 ] )\n",
    "correct_scores      = np.array( [ 1.*1. + 2.*3. + 3.*(-1.),                     1.*1. + (0.)*3. + (0.)*(-1.) ] )\n",
    "correct_first_term  = np.array( [ (correct_indicators[0]-1)*correct_scores[0],  (correct_indicators[1]-1)*correct_scores[1] ] )\n",
    "correct_second_term = np.array( [ np.log(1. + np.exp(-correct_scores[0])),      np.log(1. + np.exp(-correct_scores[1])) ] )\n",
    "\n",
    "correct_ll          =      sum( [ correct_first_term[0]-correct_second_term[0], correct_first_term[1]-correct_second_term[1] ] ) \n",
    "\n",
    "print ('The following outputs must match ')\n",
    "print ('------------------------------------------------')\n",
    "print ('correct_log_likelihood           =', correct_ll)\n",
    "print ('output of compute_log_likelihood =', compute_log_likelihood(dummy_feature_matrix, dummy_high_rating, dummy_coefficients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def logistic_regression(feature_matrix, high_rating, initial_coefficients, step_size, max_iter):\n",
    "    coefficients = np.array(initial_coefficients) # make sure it's a numpy array\n",
    "    for itr in range(max_iter):\n",
    "\n",
    "        # Predict P(y^(i) = +1|x^(i),w) using your predict_probability() function\n",
    "        # YOUR CODE HERE\n",
    "        predictions = predict_probability(feature_matrix, coefficients)\n",
    "                \n",
    "        # Compute the errors as y - predictions\n",
    "        errors = high_rating - predictions\n",
    "        for j in range(len(coefficients)): # loop over each coefficient\n",
    "            \n",
    "            # Recall that feature_matrix[:,j] is the feature column associated with coefficients[j].\n",
    "            # Compute the derivative for coefficients[j]. Save it in a variable called derivative\n",
    "            # YOUR CODE HERE\n",
    "            derivative = feature_derivative(errors, feature_matrix[:,j])\n",
    "            \n",
    "            # add the step size times the derivative to the current coefficient\n",
    "            ## YOUR CODE HERE\n",
    "            coefficients[j] = coefficients[j] + derivative*step_size\n",
    "        \n",
    "        # Checking whether log likelihood is increasing\n",
    "        if itr <= 15 or (itr <= 100 and itr % 10 == 0) or (itr <= 1000 and itr % 100 == 0) \\\n",
    "        or (itr <= 10000 and itr % 1000 == 0) or itr % 10000 == 0:\n",
    "            lp = compute_log_likelihood(feature_matrix, high_rating, coefficients)\n",
    "            print ('iteration %*d: log likelihood of observed labels = %.8f' % \\\n",
    "                (int(np.ceil(np.log10(max_iter))), itr, lp))\n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -34770.14654431\n",
      "iteration   1: log likelihood of observed labels = -34767.87459771\n",
      "iteration   2: log likelihood of observed labels = -34765.60560727\n",
      "iteration   3: log likelihood of observed labels = -34763.33956017\n",
      "iteration   4: log likelihood of observed labels = -34761.07644369\n",
      "iteration   5: log likelihood of observed labels = -34758.81624520\n",
      "iteration   6: log likelihood of observed labels = -34756.55895218\n",
      "iteration   7: log likelihood of observed labels = -34754.30455219\n",
      "iteration   8: log likelihood of observed labels = -34752.05303288\n",
      "iteration   9: log likelihood of observed labels = -34749.80438201\n",
      "iteration  10: log likelihood of observed labels = -34747.55858740\n",
      "iteration  11: log likelihood of observed labels = -34745.31563700\n",
      "iteration  12: log likelihood of observed labels = -34743.07551881\n",
      "iteration  13: log likelihood of observed labels = -34740.83822096\n",
      "iteration  14: log likelihood of observed labels = -34738.60373163\n",
      "iteration  15: log likelihood of observed labels = -34736.37203911\n",
      "iteration  20: log likelihood of observed labels = -34725.25512468\n",
      "iteration  30: log likelihood of observed labels = -34703.22456228\n",
      "iteration  40: log likelihood of observed labels = -34681.45613564\n",
      "iteration  50: log likelihood of observed labels = -34659.93984484\n",
      "iteration  60: log likelihood of observed labels = -34638.66639484\n",
      "iteration  70: log likelihood of observed labels = -34617.62714006\n",
      "iteration  80: log likelihood of observed labels = -34596.81403315\n",
      "iteration  90: log likelihood of observed labels = -34576.21957777\n",
      "iteration 100: log likelihood of observed labels = -34555.83678487\n",
      "iteration 200: log likelihood of observed labels = -34362.40365516\n",
      "iteration 300: log likelihood of observed labels = -34185.00552786\n"
     ]
    }
   ],
   "source": [
    "coefficients = logistic_regression(feature_matrix, high_rating, initial_coefficients=np.zeros(194),\n",
    "                                   step_size=1e-7, max_iter=301)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the scores as a dot product between feature_matrix and coefficients.\n",
    "scores = np.dot(feature_matrix, coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26998\n"
     ]
    }
   ],
   "source": [
    "class_pred = scores>0\n",
    "print (list(class_pred).count(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question i: How many reviews were predicted to have high rating?\n",
    "\n",
    "There are 26998 reviews that were predicted to have high ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "# Reviews   correctly classified = 33089\n",
      "# Reviews incorrectly classified = 17077\n",
      "# Reviews total                  = 50166\n",
      "-----------------------------------------------------\n",
      "Accuracy = 0.66\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "num_mistakes = list(high_rating == class_pred).count(False)\n",
    "accuracy = (len(products) - num_mistakes)/len(products)\n",
    "print (\"-----------------------------------------------------\")\n",
    "print ('# Reviews   correctly classified =', len(products) - num_mistakes)\n",
    "print ('# Reviews incorrectly classified =', num_mistakes)\n",
    "print ('# Reviews total                  =', len(products))\n",
    "print (\"-----------------------------------------------------\")\n",
    "print ('Accuracy = %.2f' % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question ii: What is the accuracy of the model on predictions made above? (round to 2 digits of accuracy)\n",
    "\n",
    "The prediction accuracy is 0.66."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coefficients = list(coefficients[1:]) # exclude intercept\n",
    "word_coefficient_tuples = [(word, coefficient) for word, coefficient in zip(important_words, coefficients)]\n",
    "word_coefficient_tuples = sorted(word_coefficient_tuples, key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('love', 0.05417175110031363),\n",
       " ('loves', 0.03341634129158078),\n",
       " ('easy', 0.03338913748001677),\n",
       " ('great', 0.025580515307137473),\n",
       " ('perfect', 0.018573255618985464),\n",
       " ('recommend', 0.018198089912211276),\n",
       " ('baby', 0.015214316424304965),\n",
       " ('best', 0.012004878798543237),\n",
       " ('daughter', 0.009724230930764759),\n",
       " ('old', 0.009607388050573889),\n",
       " ('fits', 0.00917736061196277),\n",
       " ('also', 0.009096143871771314),\n",
       " ('soft', 0.008943944922918836),\n",
       " ('happy', 0.00719076203015758),\n",
       " ('every', 0.006683151200866263),\n",
       " ('well', 0.00624658635328378),\n",
       " ('without', 0.0059343223690781904),\n",
       " ('comfortable', 0.005554796613533447),\n",
       " ('son', 0.004873375405488079),\n",
       " ('play', 0.004711611981100412),\n",
       " ('room', 0.004500995877516566),\n",
       " ('diaper', 0.004107433778957112),\n",
       " ('worth', 0.003944765282505371),\n",
       " ('many', 0.003736816435735063),\n",
       " ('months', 0.0037237525514986036),\n",
       " ('car', 0.0037204549714334573),\n",
       " ('kids', 0.0032778115843018056),\n",
       " ('clean', 0.0032011958051949255),\n",
       " ('little', 0.003179969958054386),\n",
       " ('night', 0.003082072689961903),\n",
       " ('year', 0.0030409244938375043),\n",
       " ('us', 0.0024001272043584795),\n",
       " ('easily', 0.002275536838470771),\n",
       " ('crib', 0.0021403384334870296),\n",
       " ('set', 0.0017015566795742273),\n",
       " ('size', 0.0015372045927096367),\n",
       " ('looking', 0.0014303982763842645),\n",
       " ('purchased', 0.0011362063610430516),\n",
       " ('day', 0.0010736403038844762),\n",
       " ('bottles', 0.0010670141788825323),\n",
       " ('babies', 0.0009739884494686842),\n",
       " ('works', 0.0008499924398035718),\n",
       " ('new', 0.0006104281238760828),\n",
       " ('purchase', 0.0004958060797486094),\n",
       " ('take', 0.0004428369837232663),\n",
       " ('keep', 0.000412041366303197),\n",
       " ('makes', 0.00039653557499882524),\n",
       " ('pump', 0.00027290779423743805),\n",
       " ('won', 5.2693834080007066e-05),\n",
       " ('looks', -0.00010166095079313987),\n",
       " ('month', -0.00014204354469716832),\n",
       " ('using', -0.0002374425438549246),\n",
       " ('almost', -0.000281956213081554),\n",
       " ('nice', -0.0003596997630504438),\n",
       " ('enough', -0.00036506644361972596),\n",
       " ('lot', -0.000480089333678729),\n",
       " ('right', -0.0005102686425371165),\n",
       " ('buying', -0.0005306286705218896),\n",
       " ('second', -0.0006345067026696037),\n",
       " ('since', -0.0006836343988632531),\n",
       " ('amazon', -0.0006916791150120702),\n",
       " ('go', -0.0007772731751595249),\n",
       " ('bought', -0.0007817409074797152),\n",
       " ('tub', -0.0009043998124674176),\n",
       " ('toy', -0.0009137792991781267),\n",
       " ('different', -0.0009275167226237499),\n",
       " ('able', -0.0010061385758074718),\n",
       " ('place', -0.001086457763824904),\n",
       " ('high', -0.0011054383956069292),\n",
       " ('know', -0.001310603792267647),\n",
       " ('went', -0.0013416058672562864),\n",
       " ('away', -0.0013800625007670839),\n",
       " ('cute', -0.0014613616614711103),\n",
       " ('around', -0.001505651352244034),\n",
       " ('quality', -0.001772917599933115),\n",
       " ('never', -0.001892176526282408),\n",
       " ('weeks', -0.001911551178268126),\n",
       " ('bottle', -0.001996901058178202),\n",
       " ('used', -0.0020226384700273064),\n",
       " ('found', -0.0020342536677289496),\n",
       " ('look', -0.0020449936453709196),\n",
       " ('bag', -0.0022314363960391024),\n",
       " ('much', -0.002256246447895867),\n",
       " ('long', -0.002256491240556829),\n",
       " ('times', -0.002258970640522519),\n",
       " ('getting', -0.002274170233154865),\n",
       " ('wanted', -0.0022974475056238503),\n",
       " ('made', -0.002310150151095822),\n",
       " ('started', -0.0023358631502817134),\n",
       " ('either', -0.002335906234088302),\n",
       " ('ordered', -0.0023626713599585376),\n",
       " ('came', -0.002402570061547164),\n",
       " ('milk', -0.0024028487697902935),\n",
       " ('big', -0.0024486967957327906),\n",
       " ('price', -0.002489917253913579),\n",
       " ('together', -0.0026246955140949114),\n",
       " ('need', -0.002626046282860759),\n",
       " ('say', -0.0026323167906124687),\n",
       " ('last', -0.002857973374799263),\n",
       " ('anything', -0.0029770409991956787),\n",
       " ('first', -0.0030095962948931765),\n",
       " ('chair', -0.003027125463661526),\n",
       " ('completely', -0.003408707445391459),\n",
       " ('said', -0.00349091357912835),\n",
       " ('worked', -0.003503762344963813),\n",
       " ('design', -0.0035383635034365985),\n",
       " ('took', -0.003568074514403945),\n",
       " ('less', -0.003708069020495429),\n",
       " ('come', -0.003887736093452281),\n",
       " ('child', -0.0038986799973351385),\n",
       " ('however', -0.003898998365273108),\n",
       " ('actually', -0.0039023430576277046),\n",
       " ('water', -0.004006388104535227),\n",
       " ('cover', -0.004058280159362711),\n",
       " ('hold', -0.004146709927536339),\n",
       " ('head', -0.004314267427844301),\n",
       " ('another', -0.004437482575799698),\n",
       " ('one', -0.0044692932966163175),\n",
       " ('find', -0.0044808191659420505),\n",
       " ('still', -0.004527148336667875),\n",
       " ('gate', -0.0045492763221357205),\n",
       " ('picture', -0.004575309146607962),\n",
       " ('received', -0.0046468019535556996),\n",
       " ('side', -0.004665065681902608),\n",
       " ('company', -0.004736181104253553),\n",
       " ('reviews', -0.0048039038479479644),\n",
       " ('working', -0.004872506043167943),\n",
       " ('maybe', -0.004928034431266442),\n",
       " ('give', -0.005122873137586023),\n",
       " ('item', -0.005140384576579672),\n",
       " ('idea', -0.005140934620516791),\n",
       " ('instead', -0.005157113398113809),\n",
       " ('box', -0.005220327004181242),\n",
       " ('got', -0.005349711051751324),\n",
       " ('unit', -0.005411595169075356),\n",
       " ('though', -0.005414584161840412),\n",
       " ('use', -0.0055088606752266836),\n",
       " ('open', -0.005547328478421601),\n",
       " ('want', -0.005613301559706752),\n",
       " ('see', -0.005658601113842783),\n",
       " ('trying', -0.0057738637548137),\n",
       " ('cheap', -0.0059026422708318765),\n",
       " ('part', -0.005914991685553965),\n",
       " ('returned', -0.005939635492584914),\n",
       " ('try', -0.005972097948533529),\n",
       " ('broke', -0.0060295112909709465),\n",
       " ('stroller', -0.0060746879823643934),\n",
       " ('bad', -0.0060915473748843155),\n",
       " ('tried', -0.006092921107316139),\n",
       " ('stay', -0.0062550145392366195),\n",
       " ('two', -0.006281472550694444),\n",
       " ('sure', -0.006413722740218334),\n",
       " ('going', -0.006441626199068129),\n",
       " ('piece', -0.006445490767109502),\n",
       " ('seems', -0.0064608833677325265),\n",
       " ('cup', -0.006554571937570305),\n",
       " ('fit', -0.006572254050011234),\n",
       " ('make', -0.006736193138682181),\n",
       " ('money', -0.006823483212940694),\n",
       " ('problem', -0.006856351888789802),\n",
       " ('bottom', -0.0068888241007317936),\n",
       " ('buy', -0.007150945871608536),\n",
       " ('seat', -0.00715658687539725),\n",
       " ('put', -0.007240996899317299),\n",
       " ('small', -0.007882941726412239),\n",
       " ('waste', -0.007969925874517869),\n",
       " ('time', -0.008184776872709625),\n",
       " ('top', -0.008291950229629462),\n",
       " ('wish', -0.008431362156372116),\n",
       " ('even', -0.008908289428389721),\n",
       " ('something', -0.009277656764725747),\n",
       " ('disappointed', -0.009544242907761012),\n",
       " ('thing', -0.009991832890720904),\n",
       " ('bit', -0.010540713684068392),\n",
       " ('difficult', -0.010545157935116503),\n",
       " ('return', -0.010850300211845026),\n",
       " ('plastic', -0.010891464614575101),\n",
       " ('pretty', -0.011064655969059027),\n",
       " ('think', -0.011607416182415974),\n",
       " ('better', -0.01171893785837706),\n",
       " ('way', -0.01200636517389574),\n",
       " ('monitor', -0.01212466087136327),\n",
       " ('thought', -0.012664530663116253),\n",
       " ('good', -0.01460154255474313),\n",
       " ('really', -0.014670316455461472),\n",
       " ('hard', -0.015237868674725085),\n",
       " ('back', -0.016009093804293415),\n",
       " ('work', -0.016221373513787968),\n",
       " ('product', -0.017354095077474443),\n",
       " ('could', -0.018893796476944138),\n",
       " ('get', -0.023544894371514233),\n",
       " ('like', -0.031072720033253332),\n",
       " ('would', -0.045868342428508284)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_coefficient_tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question iii: What are the top 3 most positively weighted words (according to our model)?\n",
    "\n",
    "According to our model, the three most positively weighted words are \"love\", \"loves\", \"easy\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3 Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a\n",
    "\n",
    "If the weak learning assumption holds, we have $\\epsilon_t = \\frac{1}{2}-\\gamma_t$, and $\\gamma_t>\\gamma_{WLA}$,  for all t.\n",
    "\n",
    "$\\begin{aligned}\n",
    "R^\\text{train}(\\lambda_{t+1}) &= R^\\text{train}(\\lambda_t + \\alpha_t e_{j_t})\\\\\n",
    "&= \\frac{1}{n}\\sum_{i=1}^n e^{-[M(\\lambda_t + \\alpha_t e_{j_t})]_i}\\\\\n",
    "&= e^{-\\alpha_t} \\frac{1}{n}\\sum_{i:M_{ij_t =1}}e^{-(M\\lambda_t)_i}+e^{\\alpha_t} \\frac{1}{n}\\sum_{i:M_{ij_t =-1}}e^{-(M\\lambda_t)_i}\n",
    "\\end{aligned}$\n",
    "\n",
    "We have $d_{t,i} = e^{-(M\\lambda_t)_i}/Z_t$ where $Z_t = \\sum_{i=1}^n e^{-(M\\lambda_t)_i}$.\n",
    "So $R^\\text{train}(\\lambda_{t+1}) = R^\\text{train}(\\lambda_{t})[e^{-\\alpha} d_+ + e^{\\alpha} d_-]$\n",
    "\n",
    "We have $\\alpha_t = \\frac{1}{2}\\ln (\\frac{1-d_-}{d_-}) = \\frac{1}{2}\\ln (\\frac{1-\\epsilon_t}{\\epsilon_t})$.\n",
    "\n",
    "$\\begin{aligned}\n",
    "R^\\text{train}(\\lambda_{t+1}) &= R^\\text{train}(\\lambda_{t})[e^{-\\alpha}(1-d_-) + e^{\\alpha} d_-]\\\\\n",
    "&= \\frac{d_-}{1-d_-}(1-d_-)+(\\frac{d_-}{1-d_-})^{\\frac{1}{2}}\\\\\n",
    "&= R^\\text{train}2[\\epsilon_t(1-\\epsilon_t)]^\\frac{1}{2}\n",
    "\\end{aligned}$\n",
    "\n",
    "Start with $\\lambda_1 = 0$, we have $R^\\text{train}(\\lambda_{T}) = \\prod_{i=1}^T 2\\sqrt{\\epsilon_t(1-\\epsilon_t)} = \\prod_{i=1}^T 2\\sqrt{(\\frac{1}{2}-\\gamma_t)(\\frac{1}{2}+\\gamma_t)} = \\prod_{i=1}^T\\sqrt{1-4\\gamma_t^2}$. \n",
    "\n",
    "From the inequality $1+x\\leq e^x$,we have $R^\\text{train}(\\lambda_{T}) \\leq \\prod_{i=1}^T \\sqrt{e^{-4\\gamma_t^2}}= \\prod_{i=1}^Te^{-2\\gamma_t^2}= e^{-2\\sum_{i=1}^T \\gamma_t^2}\\leq e^{-2T \\gamma_{WLA}^2}$.\n",
    "\n",
    "The upper bound approaches $0$ when $T$ grows.\n",
    "\n",
    "Therefore, if the weak learning assumption holds, the boosting model eventually classifies the training set perfectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b\n",
    "\n",
    "The exponential loss is $R^\\text{train}(\\lambda) = \\sum_{i=1}^n w_i e^{-(M\\lambda)_i}$.\n",
    "\n",
    "We can do coordinate descent on the exponential loss. At each iteration, we will choose a coordinate for $lambda$, called $j_t$, and move $\\alpha_t$ in the $j_t^{th}$ direction. By doing so, each weak classifier corresponds to a direction in the space and $\\alpha_t$ corresponds to a distance in that direction. We need to find the direction $j$ which has the steepest directional derivative. Let $e_j$ be a vector with 1 in the $j^{th}$ entry and 0 elsewhere.\n",
    "\n",
    "$\\begin{aligned}\n",
    "j_t &\\in \\text{argmax}_j [-\\frac{\\partial R^\\text{train}(\\lambda_{t}+\\alpha e_j)}{\\partial \\alpha}|_{\\alpha=0}]\\\\\n",
    "&= \\text{argmax}_j [-\\frac{\\partial}{\\partial\\alpha}[\\sum_{i=1}^n w_i e^{-(M((\\lambda_{t}+\\alpha e_j))_i}]|_{\\alpha=0}]\\\\\n",
    "&= \\text{argmax}_j [-\\frac{\\partial}{\\partial\\alpha}[\\sum_{i=1}^n w_i e^{-(M\\lambda_{t})_i-\\alpha(M e_j)_i}]|_{\\alpha=0}]\\\\\n",
    "&= \\text{argmax}_j [-\\frac{\\partial}{\\partial\\alpha}[\\sum_{i=1}^n w_i e^{-(M\\lambda_{t})_i-\\alpha(M e_j)_i}]|_{\\alpha=0}]\\\\\n",
    "&= \\text{argmax}_j [-\\frac{\\partial}{\\partial\\alpha}[\\sum_{i=1}^n w_i e^{-(M\\lambda_{t})_i-\\alpha M_{ij}}]|_{\\alpha=0}]\\\\\n",
    "&= \\text{argmax}_j [\\sum_{i=1}^n w_i M_{ij} e^{-(M\\lambda_{t})_i}]\n",
    "\\end{aligned}$\n",
    "\n",
    "We initialize the weight vector $d_{t,i} = w_i$ for all $i$. \n",
    "\n",
    "We have\n",
    "$\\begin{equation}\n",
    "d_{t+1,i} = \\frac{d_{t,i}}{Z_t} \\begin{cases} e^{-\\alpha_t} &\\text{if} y_i = h_{(t)}(x_i)\\\\\n",
    "e^{\\alpha_t} &\\text{if} y_i \\neq h_{(t)}(x_i) \\end{cases}\n",
    "\\end{equation}$,\n",
    "where $Z_t = \\sum_{i=1}^n e^{-(M\\lambda_t)_i}$.\n",
    "\n",
    "Hence, we have $j_t \\in \\text{argmax}_j \\sum_{i=1}^n w_i M_{ij}d_{t,i} = \\text{argmax}_j(d^T_t M)_j$.\n",
    "\n",
    "Then we can find $\\alpha_t$ by setting the derivate to 0.\n",
    "\n",
    "$\\begin{aligned}\n",
    "0 &= \\frac{\\partial R^\\text{train}(\\lambda_{t}+\\alpha e_j)}{\\partial \\alpha} |_{\\alpha_t}\\\\\n",
    "&= \\sum_{i=1}^n w_i M_{ij} e^{-(M\\lambda_{t})_i - \\alpha_t M_{ij_t}}\\\\\n",
    "&= \\sum_{i:M_{ij_t}=1}w_i e^{-(M\\lambda_{t})_i - \\alpha_t} -  \\sum_{i:M_{ij_t}=-1}w_ie^{-(M\\lambda_{t})_i + \\alpha_t}\\\\\n",
    "&= \\sum_{i:M_{ij_t}=1}w_i d_{t,i} e^{-\\alpha_t} -  \\sum_{i:M_{ij_t}=-1}w_i d_{t,i} e^{\\alpha_t}\\\\\n",
    "&= d_+ e^{-\\alpha_t} - d_- e^{\\alpha_t}\n",
    "\\end{aligned}$\n",
    "\n",
    "Hence, $\\alpha_t = \\frac{1}{2}\\ln\\frac{d_+}{d_-}$.\n",
    "\n",
    "So the weighted version of AdaBoost is:\n",
    "\n",
    "$d_{t,i} = w_i$ for all $i$\n",
    "\n",
    "$\\lambda_1 = 0$\n",
    "\n",
    "loop from $t = 1$ to $T$\n",
    "\n",
    "$j_t \\in \\text{argmax}_j(d^T_t M)_j$\n",
    "\n",
    "$d_- = \\sum_{i:M_{ij_t}=-1}w_i d_{t,i}$\n",
    "\n",
    "$\\alpha_t = \\frac{1}{2}\\ln\\frac{1-d_-}{d_-}$\n",
    "\n",
    "$\\lambda_{t+1} = \\lambda_t + \\alpha_t e_{j_t}$\n",
    "\n",
    "$d_{t+1,i} = \\frac{e^{-(M+\\lambda_{t=1})_i}}{Z_{t+1}}$\n",
    "\n",
    "end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loans = pd.read_csv('loan_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['grade',              # grade of the loan\n",
    "            'term',               # the term of the loan\n",
    "            'home_ownership',     # home ownership status: own, mortgage or rent\n",
    "            'emp_length',         # number of years of employment\n",
    "           ]\n",
    "\n",
    "loans['safe_loans'] = loans['loan_status'].apply(lambda x : +1 if x=='Fully Paid' else -1)\n",
    "\n",
    "## please update pandas to the newest version in order to execute the following line\n",
    "loans.drop(['loan_status'], axis = 1, inplace=True)\n",
    "\n",
    "target = 'safe_loans' # this variable will be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['term_ 36 months',\n",
       " 'term_ 60 months',\n",
       " 'grade_A',\n",
       " 'grade_B',\n",
       " 'grade_C',\n",
       " 'grade_D',\n",
       " 'grade_E',\n",
       " 'grade_F',\n",
       " 'grade_G',\n",
       " 'home_ownership_MORTGAGE',\n",
       " 'home_ownership_NONE',\n",
       " 'home_ownership_OTHER',\n",
       " 'home_ownership_OWN',\n",
       " 'home_ownership_RENT',\n",
       " 'emp_length_1 year',\n",
       " 'emp_length_10+ years',\n",
       " 'emp_length_2 years',\n",
       " 'emp_length_3 years',\n",
       " 'emp_length_4 years',\n",
       " 'emp_length_5 years',\n",
       " 'emp_length_6 years',\n",
       " 'emp_length_7 years',\n",
       " 'emp_length_8 years',\n",
       " 'emp_length_9 years',\n",
       " 'emp_length_< 1 year']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans = pd.get_dummies(loans)\n",
    "features = list(loans.columns)\n",
    "features.remove('safe_loans')  # Remove the response variable\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(loans, test_size = 0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def intermediate_node_weighted_mistakes(labels_in_node, data_weights):\n",
    "    # Sum the weights of all entries with label +1\n",
    "    total_weight_positive = np.sum(data_weights[labels_in_node == +1])\n",
    "    \n",
    "    # Sum the weights of all entries with label -1\n",
    "    ### YOUR CODE HERE\n",
    "    total_weight_negative = np.sum(data_weights[labels_in_node == -1])\n",
    "    \n",
    "    # Return the tuple (weight, class_label) representing the lower of the two weights\n",
    "    #    class_label should be an integer of value +1 or -1.\n",
    "    # If the two weights are identical, return (weighted_mistakes_all_positive,+1)\n",
    "    ### YOUR CODE HERE\n",
    "    if (total_weight_positive <= total_weight_negative):\n",
    "        return(total_weight_positive, -1)\n",
    "    else:\n",
    "        return (total_weight_negative, +1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "example_labels = pd.Series([-1, -1, 1, 1, 1])\n",
    "example_data_weights = pd.Series([1., 2., .5, 1., 1.])\n",
    "if intermediate_node_weighted_mistakes(example_labels, example_data_weights) == (2.5, -1):\n",
    "    print ('Test passed!')\n",
    "else:\n",
    "    print ('Test failed... try again!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If the data is identical in each feature, this function should return None\n",
    "\n",
    "def best_splitting_feature(data, features, target, data_weights):\n",
    "    \n",
    "    # These variables will keep track of the best feature and the corresponding error\n",
    "    best_feature = None\n",
    "    best_error = float('+inf') \n",
    "    num_points = float(len(data))\n",
    "\n",
    "    # Loop through each feature to consider splitting on that feature\n",
    "    for feature in features:\n",
    "        \n",
    "        # The left split will have all data points where the feature value is 0\n",
    "        # The right split will have all data points where the feature value is 1\n",
    "        left_split = data[data[feature] == 0]\n",
    "        right_split = data[data[feature] == 1]\n",
    "        \n",
    "        # Apply the same filtering to data_weights to create left_data_weights, right_data_weights\n",
    "        ## YOUR CODE HERE\n",
    "        left_data_weights = data_weights[data[feature] == 0]\n",
    "        right_data_weights = data_weights[data[feature] == 1] \n",
    "        \n",
    "        # Calculate the weight of mistakes for left and right sides\n",
    "        ## YOUR CODE HERE\n",
    "        left_weighted_mistakes, left_class = intermediate_node_weighted_mistakes(left_split[target], left_data_weights)\n",
    "        right_weighted_mistakes, right_class = intermediate_node_weighted_mistakes(right_split[target], right_data_weights)\n",
    "        \n",
    "        # Compute weighted error by computing\n",
    "        #  ( [weight of mistakes (left)] + [weight of mistakes (right)] ) / [total weight of all data points]\n",
    "        ## YOUR CODE HERE\n",
    "        error = (left_weighted_mistakes + right_weighted_mistakes) / (sum(left_data_weights) + sum(right_data_weights))\n",
    "        \n",
    "        # If this is the best error we have found so far, store the feature and the error\n",
    "        if error < best_error:\n",
    "            best_feature = feature\n",
    "            best_error = error\n",
    "    \n",
    "    # Return the best feature we found\n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "example_data_weights = np.array(len(train_data)* [1.5])\n",
    "if best_splitting_feature(train_data, features, target, example_data_weights) == 'term_ 36 months':\n",
    "    print ('Test passed!')\n",
    "else:\n",
    "    print ('Test failed... try again!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_leaf(target_values, data_weights):\n",
    "    \n",
    "    # Create a leaf node\n",
    "    leaf = {'splitting_feature' : None,\n",
    "            'is_leaf': True}\n",
    "    \n",
    "    # Computed weight of mistakes.\n",
    "    weighted_error, best_class = intermediate_node_weighted_mistakes(target_values, data_weights)\n",
    "    # Store the predicted class (1 or -1) in leaf['prediction']\n",
    "    ## YOUR CODE HERE\n",
    "    leaf['prediction'] = best_class\n",
    "    \n",
    "    return leaf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weighted_decision_tree_create(data, features, target, data_weights, current_depth = 1, max_depth = 10):\n",
    "    remaining_features = features[:] # Make a copy of the features.\n",
    "    target_values = data[target]\n",
    "    print (\"--------------------------------------------------------------------\")\n",
    "    print (\"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values)))\n",
    "    \n",
    "    # Stopping condition 1. Error is 0.\n",
    "    if intermediate_node_weighted_mistakes(target_values, data_weights)[0] <= 1e-15:\n",
    "        print (\"Stopping condition 1 reached.\")                \n",
    "        return create_leaf(target_values, data_weights)\n",
    "    \n",
    "    # Stopping condition 2. No more features.\n",
    "    if remaining_features == []:\n",
    "        print (\"Stopping condition 2 reached.\" )               \n",
    "        return create_leaf(target_values, data_weights)    \n",
    "    \n",
    "    # Additional stopping condition (limit tree depth)\n",
    "    if current_depth > max_depth:\n",
    "        print (\"Reached maximum depth. Stopping for now.\")\n",
    "        return create_leaf(target_values, data_weights)\n",
    "    \n",
    "    # If all the datapoints are the same, splitting_feature will be None. Create a leaf\n",
    "    splitting_feature = best_splitting_feature(data, features, target, data_weights)\n",
    "    remaining_features.remove(splitting_feature)\n",
    "        \n",
    "    left_split = data[data[splitting_feature] == 0]\n",
    "    right_split = data[data[splitting_feature] == 1]\n",
    "    \n",
    "    left_data_weights = data_weights[data[splitting_feature] == 0]\n",
    "    right_data_weights = data_weights[data[splitting_feature] == 1]\n",
    "    \n",
    "    print (\"Split on feature %s. (%s, %s)\" % (\\\n",
    "              splitting_feature, len(left_split), len(right_split)))\n",
    "    \n",
    "    # Create a leaf node if the split is \"perfect\"\n",
    "    if len(left_split) == len(data):\n",
    "        print (\"Creating leaf node.\")\n",
    "        return create_leaf(left_split[target], data_weights)\n",
    "    if len(right_split) == len(data):\n",
    "        print (\"Creating leaf node.\")\n",
    "        return create_leaf(right_split[target], data_weights)\n",
    "    \n",
    "    # Repeat (recurse) on left and right subtrees\n",
    "    left_tree = weighted_decision_tree_create(left_split, remaining_features, target, left_data_weights, current_depth + 1, max_depth)\n",
    "    right_tree = weighted_decision_tree_create(right_split, remaining_features, target, right_data_weights, current_depth + 1, max_depth)\n",
    "    \n",
    "    return {'is_leaf'          : False, \n",
    "            'prediction'       : None,\n",
    "            'splitting_feature': splitting_feature,\n",
    "            'left'             : left_tree, \n",
    "            'right'            : right_tree}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_nodes(tree):\n",
    "    if tree['is_leaf']:\n",
    "        return 1\n",
    "    return 1 + count_nodes(tree['left']) + count_nodes(tree['right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature term_ 36 months. (8850, 23150)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8850 data points).\n",
      "Split on feature grade_A. (8775, 75)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (8775 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (75 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23150 data points).\n",
      "Split on feature grade_D. (19331, 3819)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (19331 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (3819 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "example_data_weights = np.array([1.0 for i in range(len(train_data))])\n",
    "small_data_decision_tree = weighted_decision_tree_create(train_data, features, target,\n",
    "                                        example_data_weights, max_depth=2)\n",
    "if count_nodes(small_data_decision_tree) == 7:\n",
    "    print ('Test passed!')\n",
    "else:\n",
    "    print ('Test failed... try again!')\n",
    "    print ('Number of nodes found:', count_nodes(small_data_decision_tree))\n",
    "    print ('Number of nodes that should be there: 7') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_leaf': False,\n",
       " 'left': {'is_leaf': False,\n",
       "  'left': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
       "  'prediction': None,\n",
       "  'right': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
       "  'splitting_feature': 'grade_A'},\n",
       " 'prediction': None,\n",
       " 'right': {'is_leaf': False,\n",
       "  'left': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
       "  'prediction': None,\n",
       "  'right': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
       "  'splitting_feature': 'grade_D'},\n",
       " 'splitting_feature': 'term_ 36 months'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data_decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify(tree, x, annotate = False):   \n",
    "    # If the node is a leaf node.\n",
    "    if tree['is_leaf']:\n",
    "        if annotate: \n",
    "            print (\"At leaf, predicting %s\" % tree['prediction'])\n",
    "        return tree['prediction'] \n",
    "    else:\n",
    "        # Split on feature.\n",
    "        split_feature_value = x[tree['splitting_feature']]\n",
    "        if annotate: \n",
    "            print (\"Split on %s = %s\" % (tree['splitting_feature'], split_feature_value))\n",
    "        if split_feature_value == 0:\n",
    "            return classify(tree['left'], x, annotate)\n",
    "        else:\n",
    "            return classify(tree['right'], x, annotate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_classification_error(tree, data):\n",
    "    # Apply the classify(tree, x) to each row in your data\n",
    "    prediction = data.apply(lambda x: classify(tree, x), axis = 1)\n",
    "    \n",
    "    # Once you've made the predictions, calculate the classification error\n",
    "    return (prediction != data[target]).sum() / float(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.390875"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(small_data_decision_tree, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature emp_length_10+ years. (22413, 9587)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (22413 data points).\n",
      "Split on feature grade_A. (19673, 2740)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (19673 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (2740 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9587 data points).\n",
      "Stopping condition 1 reached.\n"
     ]
    }
   ],
   "source": [
    "# Assign weights\n",
    "example_data_weights = np.array([1.] * 10 + [0.]*(len(train_data) - 20) + [1.] * 10)\n",
    "\n",
    "# Train a weighted decision tree model.\n",
    "small_data_decision_tree_subset_20 = weighted_decision_tree_create(train_data, features, target,\n",
    "                         example_data_weights, max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_20 = train_data.head(10).append(train_data.tail(10))\n",
    "evaluate_classification_error(small_data_decision_tree_subset_20, subset_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.445625"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(small_data_decision_tree_subset_20, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "from math import exp\n",
    "\n",
    "def adaboost_with_tree_stumps(data, features, target, num_tree_stumps):\n",
    "    # start with unweighted data (uniformly weighted)\n",
    "    alpha =  np.array([1.]*len(data))\n",
    "    weights = []\n",
    "    tree_stumps = []\n",
    "    target_values = data[target]\n",
    "    \n",
    "    for t in range(num_tree_stumps):\n",
    "        print ('=====================================================')\n",
    "        print ('Adaboost Iteration %d' % t)\n",
    "        print ('=====================================================')        \n",
    "        # Learn a weighted decision tree stump. Use max_depth=1\n",
    "        # YOUR CODE HERE\n",
    "        tree_stump = weighted_decision_tree_create(data, features, target, data_weights=alpha, max_depth=1)\n",
    "        tree_stumps.append(tree_stump)\n",
    "        \n",
    "        # Make predictions\n",
    "        ## YOUR CODE HERE\n",
    "        predictions = data.apply(lambda x: classify(tree_stump, x), axis = 1)\n",
    "        \n",
    "        # Produce a Boolean array indicating whether\n",
    "        # each data point was correctly classified\n",
    "        is_correct = predictions == target_values\n",
    "        is_wrong   = predictions != target_values\n",
    "        \n",
    "        # Compute weighted error\n",
    "        ## YOUR CODE HERE\n",
    "        weighted_error = sum(alpha[is_wrong]) / sum(alpha)\n",
    "        \n",
    "        # Compute model coefficient using weighted error\n",
    "        ## YOUR CODE HERE\n",
    "        weight = 0.5*log((1 - weighted_error) / weighted_error)\n",
    "        weights.append(weight)\n",
    "        \n",
    "        # Adjust weights on data point\n",
    "        ## YOUR CODE HERE\n",
    "        adjustment = is_correct.apply(lambda is_correct : exp(-weight) if is_correct else exp(weight))\n",
    "        \n",
    "        # Scale alpha by multiplying by adjustment \n",
    "        # Then normalize data points weights\n",
    "        ## YOUR CODE HERE \n",
    "        alpha = alpha * adjustment\n",
    "        alpha = alpha/sum(alpha)\n",
    "    \n",
    "    return weights, tree_stumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature term_ 36 months. (8850, 23150)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8850 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23150 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    }
   ],
   "source": [
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, features, target, num_tree_stumps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_stump(tree):\n",
    "    split_name = tree['splitting_feature'] # split_name is something like 'term. 36 months'\n",
    "    if split_name is None:\n",
    "        print (\"(leaf, label: %s)\" % tree['prediction'])\n",
    "        return None\n",
    "    split_feature, split_value = split_name.split('_')\n",
    "    print ('                       root')\n",
    "    print ('         |---------------|----------------|')\n",
    "    print ('         |                                |')\n",
    "    print ('         |                                |')\n",
    "    print ('         |                                |')\n",
    "    print ('  [{0} == 0]{1}[{0} == 1]    '.format(split_name, ' '*(27-len(split_name))))\n",
    "    print ('         |                                |')\n",
    "    print ('         |                                |')\n",
    "    print ('         |                                |')\n",
    "    print ('    (%s)                 (%s)' \\\n",
    "        % (('leaf, label: ' + str(tree['left']['prediction']) if tree['left']['is_leaf'] else 'subtree'),\n",
    "           ('leaf, label: ' + str(tree['right']['prediction']) if tree['right']['is_leaf'] else 'subtree')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [term_ 36 months == 0]            [term_ 36 months == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (leaf, label: -1)                 (leaf, label: 1)\n"
     ]
    }
   ],
   "source": [
    "print_stump(tree_stumps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [grade_A == 0]                    [grade_A == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (leaf, label: -1)                 (leaf, label: 1)\n"
     ]
    }
   ],
   "source": [
    "print_stump(tree_stumps[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17198848113764034, 0.1772878063726963]\n"
     ]
    }
   ],
   "source": [
    "print (stump_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature term_ 36 months. (8850, 23150)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8850 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23150 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 2\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_D. (26027, 5973)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26027 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5973 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 3\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_B. (23457, 8543)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23457 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8543 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 4\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_E. (28766, 3234)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28766 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3234 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 5\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (16870, 15130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (16870 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (15130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 6\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 7\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_F. (30624, 1376)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30624 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1376 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 8\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 9\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_E. (28766, 3234)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28766 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3234 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    }
   ],
   "source": [
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, features, \n",
    "                                target, num_tree_stumps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_adaboost(stump_weights, tree_stumps, data):\n",
    "    scores = np.array([0.]*len(data))\n",
    "    \n",
    "    for i, tree_stump in enumerate(tree_stumps):\n",
    "        predictions = data.apply(lambda x: classify(tree_stump, x), axis = 1)\n",
    "        \n",
    "        # Accumulate predictions on scaores array\n",
    "        # YOUR CODE HERE\n",
    "        scores += (predictions*stump_weights[i])\n",
    "        \n",
    "    return scores.apply(lambda score : +1 if score > 0 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 10-component ensemble = 0.62825\n"
     ]
    }
   ],
   "source": [
    "predictions = predict_adaboost(stump_weights, tree_stumps, test_data)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(test_data[target], predictions)\n",
    "print ('Accuracy of 10-component ensemble = %s' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17198848113764034, 0.1772878063726963, 0.10308067697010909, 0.08686702058336851, 0.07220085937793974, 0.07438562925258671, 0.05834552873244469, 0.04545487026475097, 0.0319454846001187, 0.023305292432239024]\n"
     ]
    }
   ],
   "source": [
    "print (stump_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question i: Are the weights monotonically decreasing, monotonically increasing, or neither?\n",
    "\n",
    "The weights are monotonically decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature term_ 36 months. (8850, 23150)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8850 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23150 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 2\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_D. (26027, 5973)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26027 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5973 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 3\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_B. (23457, 8543)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23457 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8543 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 4\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_E. (28766, 3234)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28766 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3234 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 5\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (16870, 15130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (16870 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (15130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 6\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 7\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_F. (30624, 1376)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30624 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1376 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 8\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 9\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_E. (28766, 3234)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28766 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3234 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 10\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature term_ 36 months. (8850, 23150)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8850 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23150 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 11\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_F. (30624, 1376)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30624 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1376 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 12\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature emp_length_10+ years. (22413, 9587)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (22413 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9587 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 13\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_B. (23457, 8543)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23457 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8543 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 14\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_F. (30624, 1376)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30624 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1376 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 15\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_D. (26027, 5973)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26027 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5973 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 16\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_F. (30624, 1376)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30624 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1376 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 17\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 18\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_E. (28766, 3234)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28766 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3234 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 19\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_C. (23388, 8612)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23388 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8612 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 20\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (16870, 15130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (16870 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (15130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 21\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature term_ 36 months. (8850, 23150)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8850 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23150 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 22\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_F. (30624, 1376)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30624 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1376 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 23\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_B. (23457, 8543)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23457 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8543 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 24\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature emp_length_2 years. (29104, 2896)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (29104 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (2896 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 25\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_G. (31657, 343)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (31657 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (343 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 26\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 27\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_G. (31657, 343)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (31657 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (343 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 28\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature home_ownership_OWN. (29204, 2796)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (29204 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (2796 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 29\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_G. (31657, 343)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (31657 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (343 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    }
   ],
   "source": [
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, \n",
    "                                 features, target, num_tree_stumps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, training error = 0.41484374999999996\n",
      "Iteration 2, training error = 0.43281250000000004\n",
      "Iteration 3, training error = 0.39059374999999996\n",
      "Iteration 4, training error = 0.39059374999999996\n",
      "Iteration 5, training error = 0.37931250000000005\n",
      "Iteration 6, training error = 0.38228125\n",
      "Iteration 7, training error = 0.37253125\n",
      "Iteration 8, training error = 0.37549999999999994\n",
      "Iteration 9, training error = 0.37253125\n",
      "Iteration 10, training error = 0.37253125\n",
      "Iteration 11, training error = 0.37253125\n",
      "Iteration 12, training error = 0.37150000000000005\n",
      "Iteration 13, training error = 0.37253125\n",
      "Iteration 14, training error = 0.37150000000000005\n",
      "Iteration 15, training error = 0.37150000000000005\n",
      "Iteration 16, training error = 0.37150000000000005\n",
      "Iteration 17, training error = 0.37150000000000005\n",
      "Iteration 18, training error = 0.37146875\n",
      "Iteration 19, training error = 0.37150000000000005\n",
      "Iteration 20, training error = 0.37146875\n",
      "Iteration 21, training error = 0.37209375\n",
      "Iteration 22, training error = 0.37146875\n",
      "Iteration 23, training error = 0.37212500000000004\n",
      "Iteration 24, training error = 0.37150000000000005\n",
      "Iteration 25, training error = 0.37150000000000005\n",
      "Iteration 26, training error = 0.37212500000000004\n",
      "Iteration 27, training error = 0.37150000000000005\n",
      "Iteration 28, training error = 0.37131250000000005\n",
      "Iteration 29, training error = 0.37121875000000004\n",
      "Iteration 30, training error = 0.37124999999999997\n"
     ]
    }
   ],
   "source": [
    "error_all = []\n",
    "for n in range(1, 31):\n",
    "    predictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], train_data)\n",
    "    error = 1.0 - accuracy_score(train_data[target], predictions)\n",
    "    error_all.append(error)\n",
    "    print (\"Iteration %s, training error = %s\" % (n, error_all[n-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdoAAAFgCAYAAAActbi8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f66e41b7d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = 7, 5\n",
    "plt.plot(range(1,31), error_all, '-', linewidth=4.0, label='Training error')\n",
    "plt.title('Performance of Adaboost ensemble')\n",
    "plt.xlabel('# of iterations')\n",
    "plt.ylabel('Classification error')\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "\n",
    "plt.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, test error = 0.41037500000000005\n",
      "Iteration 2, test error = 0.43174999999999997\n",
      "Iteration 3, test error = 0.390875\n",
      "Iteration 4, test error = 0.390875\n",
      "Iteration 5, test error = 0.37825\n",
      "Iteration 6, test error = 0.382625\n",
      "Iteration 7, test error = 0.37175\n",
      "Iteration 8, test error = 0.37612500000000004\n",
      "Iteration 9, test error = 0.37175\n",
      "Iteration 10, test error = 0.37175\n",
      "Iteration 11, test error = 0.37175\n",
      "Iteration 12, test error = 0.369375\n",
      "Iteration 13, test error = 0.369375\n",
      "Iteration 14, test error = 0.369375\n",
      "Iteration 15, test error = 0.369375\n",
      "Iteration 16, test error = 0.369375\n",
      "Iteration 17, test error = 0.369375\n",
      "Iteration 18, test error = 0.371\n",
      "Iteration 19, test error = 0.369375\n",
      "Iteration 20, test error = 0.371\n",
      "Iteration 21, test error = 0.36924999999999997\n",
      "Iteration 22, test error = 0.371\n",
      "Iteration 23, test error = 0.367625\n",
      "Iteration 24, test error = 0.369375\n",
      "Iteration 25, test error = 0.369375\n",
      "Iteration 26, test error = 0.367625\n",
      "Iteration 27, test error = 0.369375\n",
      "Iteration 28, test error = 0.36950000000000005\n",
      "Iteration 29, test error = 0.369\n",
      "Iteration 30, test error = 0.369\n"
     ]
    }
   ],
   "source": [
    "test_error_all = []\n",
    "for n in range(1, 31):\n",
    "    predictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], test_data)\n",
    "    error = 1.0 - accuracy_score(test_data[target], predictions)\n",
    "    test_error_all.append(error)\n",
    "    print (\"Iteration %s, test error = %s\" % (n, test_error_all[n-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAFTCAYAAAAKvWRNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f66e213d908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = 7, 5\n",
    "plt.plot(range(1,31), error_all, '-', linewidth=4.0, label='Training error')\n",
    "plt.plot(range(1,31), test_error_all, '-', linewidth=4.0, label='Test error')\n",
    "\n",
    "plt.title('Performance of Adaboost ensemble')\n",
    "plt.xlabel('# of iterations')\n",
    "plt.ylabel('Classification error')\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question ii: From this plot (with 30 trees), is there massive overfitting as the # of iterations increases?\n",
    "\n",
    "As the number of iterations increases, the training error is slightly higher than testing error, so there is no overfitting. And we should note that after about 15 iterations, there is not much improvement in classification error."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
